---
title: MCTS (Deep Mind Edition)
layout: post
post-image: "https://github.com/ZoeyZwee/zoeyzwee.github.io/blob/master/assets/images/MCTS.png?raw=true"
description: A deep-dive into the Monte Carlo Tree Search, and the specific variants used by Deep Mind in AlphaGo, AlphaZero, MuZero, etc. 
tags:
- monte carlo tree search
- mcts
- reinforcement learning
- deep mind
- alphazero
- muzero
- uct
---

text 1
> ##### A note from the author:
> This article was first written for my personal notes when doing research for my project [Py2048-ML v2](https://github.com/ZoeyZwee/Py2048-ML-v2). It is assumed that readers already have some experience with the fundamentals of reinforcement learning, such as states, environments, value functions, policies, etc. If you wish to learn about these ideas, I highly recommend [this lecture series by David Silver](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ).


# What is MCTS?
Monte Carlo Tree Search (MCTS) is a scheme for approximating the action-value function of a game-tree. It aims to improve on Monte Carlo policy evaluation, which suffers from very slow convergence due to needing to trajectories to completion. It overcomes this by only evaluating partial trajectories, then estimating the value for the remainder of the trajectory. 

Implementing MCTS requires the following components:
- A model of the environment (or simulator) that outputs the next state (or resulting afterstate) and transition reward associated with taking a state-action pair.
- A selection scheme for choosing actions in the "slow" part of the tree (eg. PUCT). Usually this is done by assigning an "MCTS score" to each state (or state-action pair), then acting greedily wrt. to the MCTS score.
- A scheme for assigning initial value estimates to new states as we encounter them for the first time (eg. rollouts). These value estimates will be used to save us having to play out entire trajectories, since we can just stop at a certain point and estimate the value of the remainder.

Common approximation techniques are rollouts and neural networks. With rollouts, we play out games according to some fast policy (random moves, greedy, etc.), and set the value of the leaf to be the mean reward acquired during rollout. With a neural network, and use that.

> Note: I will use the term "node" to interchangeably refer to a state, state-action pair, or afterstate. There is some technical difference between the 3 ideas, but for the purposes of MCTS it mostly comes down to implementation detail.

We estimate the value of nodes by sampling trajectories through the game tree. The value of a node is simply average reward acquired after visiting that node in each trajectory. Each trajectory consists of 2 phases: the in-tree phase, and the after-tree phase.  
The in-tree phase consists of nodes which have previously been visited in the search. In this phase, actions are selected according to MCTS score. The reward accumulated during this phase comes from the actual transition rewards given by the model (or simulator).  
The after-tree phases begins when we reach a leaf node: either a state-action pair that hasn't been taken, or a state that hasn't been visited, depending on scheme. At this point, we use the value approximation scheme to assign an initial value to this leaf. 

Exploring a trajectory consists of 4 steps:
1. Selection. We use the selection scheme to choose actions and navigate through the tree, keeping track of any transition rewards we accumulate. Selection stops when we encounter a leaf. 
2. Expansion. We expand the leaf, adding its children to the tree. We use the value approximation scheme to assign initial values to the children. This might be rollouts or neural network evaluation. 
3. Backup. For each node in the selected path, increase its total value by the (potentially discounted) value of the newly expanded leaf, and any transition rewards that were accumulated *after* that node.
4. Repeat.
After exploring some number of trajectories (or exploring for some fixed amount of time), the search terminates. The selected action (from the root) is the one which was taken the most number of times. Final value estimates for each node are given by their total value, divided by the number of times that node was visited.

# DeepMind's MCTS, Summarized
Deep Mind uses the same general scheme for its MCTS in AlphaZero, MuZero etc. 

Overview from AlphaZero Paper:
> Each search (i.e. determination of the "best" move) consists of a series of simulated games of self-play that traverse a tree from root $s_{root}$ to leaf. Each simulation proceeds by selecting in each state $s$ a move $a$ with low visit count, high move probability and high value (averaged over the leaf states of simulations that selected $a$ from $s$) according to the current neural network $f_Î¸$. The search returns a vector $\pi$ representing a probability distribution over moves, either proportionally or greedily with respect to the visit counts at the root state.

In AlphaGo and AlphaZero, environment model is actually a simulator with perfect knowledge of the state transitions. These networks were only used on zero-sum games, so no transition rewards exist.
In MuZero, the environment model is learned, and allows for transition rewards.

The value approximation and selection schemes both depend on the output of a neural network. This network takes in a state and outputs 1. the value of that state, and 2. the probability that it would take each action in that state (i.e. a policy)
For value approximation of a leaf, we simply query the network and take the value of the leaf state.
The selection scheme for the in-tree phase is PUCT, which mixes the sum of returns found during MCTS and the networks current policy. The MCTS score in PUCT is given as follows: $PUCT(s,a) = Q(s,a) + c*p(s,a)*\frac{\sqrt{parent.N}}{child.N + 1}$ where 
- $Q(s,a)$ is the current monte-carlo estimate of the node's value (i.e. the avg return of trajectories in this search). Note while this includes value estimates given by the neural network, it also includes real rewards acquired while exploring trajectories (either transition rewards or terminal rewards).
- $c$ is a tunable parameter between 0 and 1 (higher = more exploration)
- $p(s,a)$ is the prior probability of the action being selected, according to the policy network
- parent.N is the number of times the current state has been visited
- child.N is the number of times the afterstate of the pair $(s,a)$ has been visited.

To play a game, we repeatedly use MCTS to select actions, each time discarding the tree and stating fresh. Each completed MCTS returns a value estimate for the root node, and counts for how many times we took each possible action, from the root node. We can use the value estimate as a "value label" for the root state, and the counts as a "policy label" for the root state (since the counts can be normalized and interpreted as a probability distribution). These labels can then be used as targets to train the network.

## Full explanation from MuZero Paper...
We now describe the search algorithm used by MuZero. Our approach is based upon Monte-Carlo tree search with upper confidence bounds, an approach to planning that converges asymptotically to the optimal policy in single agent domains and to the minimax value function in zero sum games.

Every node of the search tree is associated with an internal state $s$.
For each action $a$ from $s$ there is an edge $(s,a)$ that stores a set of statistics:
- $N(s,a)$ - visit counts
- $Q(s,a)$ - mean value
- $R(s,a)$ - transition reward
- $S(s,a)$ - state transition

Similar to AlphaZero, the search is divided into three stages, repeated for a number of simulations.

**Selection:** Each simulation starts from the internal root state $s^0$, and finishes when the simulation reaches a leaf node $s^l$. For each hypothetical time-step $k = 1 ... l$ of the simulation, an action $a^k$ is selected according to the stored statistics for internal state $s^{k-1}$, by maximizing over an upper confidence bound.,

$$
a^{k} = \arg\max_{a}\bigg[
Q(s, a) + P(s, a) \cdot \frac{\sqrt{\sum_b N(s, b)}}{1 + N(s, a)} \bigg(c_1 + \log\Big(\frac{\sum_b N(s, b) + c_2 + 1}{c_2}\Big)\bigg) \bigg]
$$

The constants $$c_1$ and $c_2$ are used to control the influence of the prior $P(s, a)$ relative to the value $Q(s, a)$ as nodes are visited more often. In our experiments, $c_1 = 1.25$ and $c_2 = 19652$.

For $k<l$, the next state and reward are looked up in the state transition and reward table $s^{k} = S(s^{k-1}, a^{k})$, $r^{k} = R(s^{k-1}, a^{k})$.

**Selection**: At the final time-step $l$ of the simulation, the reward and state are computed by the dynamics function, $r^l, s^l = g_\theta(s^{l-1}, a^{l})$, and stored in the corresponding tables $R(s^{l-1}, a^{l}) = r^l$ $ S(s^{l-1}, a^{l}) = s^l$. The policy and value are computed by the prediction function, $\mathbf{p}^l, v^l = f_\theta(s^l)$. A new node, corresponding to state $s^l$ is added to the search tree. Each edge $(s^l, a)$ from the newly expanded node is initialized to $\{ N(s^l, a)=0, Q(s^l, a)=0, P(s^l, a)=\mathbf{p}^l \}$. Note that the search algorithm makes at most one call to the dynamics function and prediction function respectively per simulation; the computational cost is of the same order as in AlphaZero.

**Backup:** At the end of the simulation, the statistics along the trajectory are updated. The backup is generalized to the case where the environment can emit intermediate rewards, have a discount $\gamma$ different from $1$, and the value estimates are unbounded. Note: In board games the discount is assumed to be $1$ and there are no intermediate rewards.

For $k=l ... 0$, we form an $l-k$-step estimate of the cumulative discounted reward, bootstrapping from the value function $v^l$, $ G^k = \sum_{\tau=0}^{l-1-k}{\gamma ^ \tau r_{k+ 1 +\tau}} + \gamma^{l - k} v^l $

For $k= l ... 1$, we update the statistics for each edge $(s^{k-1}, a^{k})$ in the simulation path as follows,

$$
\begin{aligned}
Q(s^{k-1},a^k) & := \frac{N(s^{k-1},a^k) \cdot Q(s^{k-1},a^k) + G^k}{N(s^{k-1},a^k) + 1} \\
N(s^{k-1},a^k) & := N(s^{k-1},a^k) + 1
\end{aligned}
$$

In two-player zero sum games the value functions are assumed to be bounded within the $[0, 1]$ interval. This choice allows us to combine value estimates with probabilities using the pUCT rule. However, since in many environments the value is unbounded, it is necessary to adjust the pUCT rule. A simple solution would be to use the maximum score that can be observed in the environment to either re-scale the value or set the pUCT constants appropriately. However, both solutions are game specific and require adding prior knowledge to the MuZero algorithm. To avoid this, computes normalized $Q$ value estimates $\overline{Q} \in [0, 1]$ by using the minimum-maximum values observed in the search tree up to that point.
When a node is reached during the selection stage, the algorithm computes the normalized $\overline{Q}$ values of its edges to be used in the pUCT rule using the equation below:

$
\overline{Q}(s^{k-1}, a^k) = \frac{Q(s^{k-1}, a^k) - \min_{s, a \in Tree}Q(s, a)}{\max_{s,a \in Tree} Q(s, a) - \min_{s,a \in Tree} Q(s, a)}
$